{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190d2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aswin Deivanayagam\\an3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 279ms/step - accuracy: 0.5421 - loss: 0.6826 - val_accuracy: 0.6022 - val_loss: 0.6692\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 256ms/step - accuracy: 0.5896 - loss: 0.6701 - val_accuracy: 0.5969 - val_loss: 0.6556\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 309ms/step - accuracy: 0.7648 - loss: 0.5005 - val_accuracy: 0.8023 - val_loss: 0.4104\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 270ms/step - accuracy: 0.8917 - loss: 0.2788 - val_accuracy: 0.8540 - val_loss: 0.3450\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 265ms/step - accuracy: 0.9366 - loss: 0.1857 - val_accuracy: 0.8400 - val_loss: 0.4242\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 221ms/step - accuracy: 0.9543 - loss: 0.1401 - val_accuracy: 0.8459 - val_loss: 0.4197\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 253ms/step - accuracy: 0.9725 - loss: 0.1028 - val_accuracy: 0.8414 - val_loss: 0.4837\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 327ms/step - accuracy: 0.9809 - loss: 0.0693 - val_accuracy: 0.8413 - val_loss: 0.5534\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 349ms/step - accuracy: 0.9883 - loss: 0.0460 - val_accuracy: 0.8348 - val_loss: 0.6107\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 363ms/step - accuracy: 0.9905 - loss: 0.0416 - val_accuracy: 0.8384 - val_loss: 0.6797\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 94ms/step - accuracy: 0.8367 - loss: 0.6868\n",
      "Test Accuracy: 83.84%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "train_data, test_data = dataset['train'], dataset['test']\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    texts, labels = [], []\n",
    "    for text, label in dataset:  \n",
    "        texts.append(text.numpy().decode('utf-8'))\n",
    "        labels.append(label.numpy())\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "\n",
    "X_train, y_train = preprocess_dataset(train_data)\n",
    "X_test, y_test = preprocess_dataset(test_data)\n",
    "\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 200  \n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 128, input_length=max_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def predict_opinion(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return \"Positive\" if prediction[0][0] >= 0.5 else \"Negative\" # Access the prediction correctly\n",
    "\n",
    "\n",
    "print(predict_opinion(\"This movie was absolutely fantastic! The acting was topnotch and the story was compelling.\"))\n",
    "print(predict_opinion(\"I hated every second of this film. The plot was nonexistent and the action was awful.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02cff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasetsNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'promise' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'promise'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Using cached etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Using cached immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (1.24.3)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (4.25.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (2.31.0)\n",
      "Collecting simple_parsing (from tensorflow-datasets)\n",
      "  Using cached simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Using cached tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2023.4.0)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.13.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.2.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from dm-tree->tensorflow-datasets) (22.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswin deivanayagam\\an3\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.6)\n",
      "Using cached tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
      "Using cached etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading dm_tree-0.1.9-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached tensorflow_metadata-1.17.1-py3-none-any.whl (31 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=225cb187bc646c348050de2922c1cb7c8c5d7efcfe327b8893566d40e0e9144a\n",
      "  Stored in directory: c:\\users\\aswin deivanayagam\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: promise, importlib_resources, immutabledict, googleapis-common-protos, etils, einops, docstring-parser, dm-tree, tensorflow-metadata, simple_parsing, tensorflow-datasets\n",
      "\n",
      "   --- ------------------------------------  1/11 [importlib_resources]\n",
      "   --- ------------------------------------  1/11 [importlib_resources]\n",
      "   ---------- -----------------------------  3/11 [googleapis-common-protos]\n",
      "   ---------- -----------------------------  3/11 [googleapis-common-protos]\n",
      "   ---------- -----------------------------  3/11 [googleapis-common-protos]\n",
      "   ---------- -----------------------------  3/11 [googleapis-common-protos]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   -------------- -------------------------  4/11 [etils]\n",
      "   ------------------ ---------------------  5/11 [einops]\n",
      "   ------------------ ---------------------  5/11 [einops]\n",
      "   --------------------- ------------------  6/11 [docstring-parser]\n",
      "   ----------------------------- ----------  8/11 [tensorflow-metadata]\n",
      "   -------------------------------- -------  9/11 [simple_parsing]\n",
      "   -------------------------------- -------  9/11 [simple_parsing]\n",
      "   -------------------------------- -------  9/11 [simple_parsing]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ------------------------------------ --- 10/11 [tensorflow-datasets]\n",
      "   ---------------------------------------- 11/11 [tensorflow-datasets]\n",
      "\n",
      "Successfully installed dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 googleapis-common-protos-1.70.0 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 simple_parsing-0.1.7 tensorflow-datasets-4.9.8 tensorflow-metadata-1.17.1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
